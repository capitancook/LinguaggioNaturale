{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/capitanio/linguaggio-naturale/blob/master/Linguaggio naturale A.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborazione del linguaggio naturale\n",
    "\n",
    "## Lettura dei quotidiani\n",
    "\n",
    "Una semplice applicazione di web scraping applicata ai quotidiani\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leggiamo il \"giornale\"\n",
    "Prepariamo tutto il necessario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "     ---------------------------------------- 0.0/211.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 211.1/211.1 kB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from newspaper3k) (10.1.0)\n",
      "Collecting PyYAML>=3.11\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "     ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 144.7/144.7 kB 8.4 MB/s eta 0:00:00\n",
      "Collecting cssselect>=0.9.2\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from newspaper3k) (2.28.1)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 81.3/81.3 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting tldextract>=2.0.1\n",
      "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 0.0/97.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 97.7/97.7 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 1.3/7.4 MB 27.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 4.8/7.4 MB 61.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.4/7.4 MB 59.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 47.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.2.post1)\n",
      "Requirement already satisfied: six in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2022.12.7)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lcapitanio\\anaconda3\\envs\\dataanalysis\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13566 sha256=769baad381a96a94a6433681fbbf74556f4d775651915d831eb6eec0edad3aab\n",
      "  Stored in directory: c:\\users\\lcapitanio\\appdata\\local\\pip\\cache\\wheels\\fc\\ab\\f8\\cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3358 sha256=558583d5f7d1c24fc080a637875a78ddb8c582095e15ec98049f336923c74aba\n",
      "  Stored in directory: c:\\users\\lcapitanio\\appdata\\local\\pip\\cache\\wheels\\80\\d5\\72\\9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398388 sha256=a02a066c5b7ee1e041478e132c587db51e0714c308a1d170dda3833c1e97e277\n",
      "  Stored in directory: c:\\users\\lcapitanio\\appdata\\local\\pip\\cache\\wheels\\3a\\a1\\46\\8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=c3af3e6821c78ee1a5f6988955ecb20e192a98c3b9b263514d1e53566397873a\n",
      "  Stored in directory: c:\\users\\lcapitanio\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, PyYAML, filelock, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed PyYAML-6.0.1 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.13.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install newspaper3k \n",
    "# decommentare e eseguire se la libreria non Ã¨ installata sulla macchina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lcapitanio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "article = Article('https://www.repubblica.it/il-gusto/2022/11/25/news/la_bagna_cauda_piace_a_tutti_da_asti_alleuropa_a_new_york-375987028/')\n",
    "article.download()\n",
    "article.parse()\n",
    "article.nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHi sono gli autori?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Di Martina Tartaglino',\n",
       " 'Di Gaetano Zoccali',\n",
       " 'Di Giovanni Mari',\n",
       " 'Di Lorenzo Cresci',\n",
       " 'Di Lara De Luna',\n",
       " 'Di Stefano Pesce']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "article.authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quali sono le parole chiavi dell'articolo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nel',\n",
       " 'che',\n",
       " 'la',\n",
       " 'bagna',\n",
       " 'tutto',\n",
       " 'cauda',\n",
       " 'shanghai',\n",
       " 'asti',\n",
       " 'sono',\n",
       " 'piace',\n",
       " 'piemonte',\n",
       " 'settimana',\n",
       " 'una',\n",
       " 'tutti',\n",
       " 'da',\n",
       " 'passando',\n",
       " 'york',\n",
       " 'solo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leggiamo il testo dell'articolo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non solo Piemonte. La bagna cauda arriva nel cuore dellâEuropa, nella Grande Mela e in una delle metropoli cinesi piÃ¹ importanti e internazionali. Infatti nel fine settimana del Bagna Cauda Day (25, 26, 27 novembre e poi 2,3,4 dicembre), lâevento che lâAssociazione culturale Astigiani organizza da dieci anni ad Asti, in Italia e nel mondo, oltre ai 150 locali piemontesi che aderiscono allâiniziativa (compresi cinque punti Eataly in tutto il Piemonte e a Bologna) ci sono anche noti ristoranti italiani allâestero.\n"
     ]
    }
   ],
   "source": [
    "print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo anche chiedere il riassunto dell'articolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non solo Piemonte.\n",
      "La bagna cauda arriva nel cuore dellâEuropa, nella Grande Mela e in una delle metropoli cinesi piÃ¹ importanti e internazionali.\n",
      "Infatti nel fine settimana del Bagna Cauda Day (25, 26, 27 novembre e poi 2,3,4 dicembre), lâevento che lâAssociazione culturale Astigiani organizza da dieci anni ad Asti, in Italia e nel mondo, oltre ai 150 locali piemontesi che aderiscono allâiniziativa (compresi cinque punti Eataly in tutto il Piemonte e a Bologna) ci sono anche noti ristoranti italiani allâestero.\n"
     ]
    }
   ],
   "source": [
    "print(article.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo tokenizzare in frasi il sommario dell'articolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output data (via Sentence Tokenizer):\n",
      "\tNon solo Piemonte. |\n",
      "\tLa bagna cauda arriva nel cuore dellâEuropa, nella Grande Mela e in una delle metropoli cinesi piÃ¹ importanti e internazionali. |\n",
      "\tInfatti nel fine settimana del Bagna Cauda Day (25, 26, 27 novembre e poi 2,3,4 dicembre), lâevento che lâAssociazione culturale Astigiani organizza da dieci anni ad Asti, in Italia e nel mondo, oltre ai 150 locali piemontesi che aderiscono allâiniziativa (compresi cinque punti Eataly in tutto il Piemonte e a Bologna) ci sono anche noti ristoranti italiani allâestero. |\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer_output = sent_tokenize(article.summary)\n",
    "print('Output data (via Sentence Tokenizer):') \n",
    "for token in sentence_tokenizer_output:    \n",
    "    print('\\t{}'.format(token),\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b777693d6eb9c5db5f44b0873b756b81181178533692d02b0d2c373d32b3107"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
